{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二章 Pytorch基础知识\n",
    "### 2.1 张量 \n",
    "本章我们开始介绍PyTorch基础知识，我们从张量说起，建立起对数据的描述，再介绍张量的运算，最后再讲PyTorch中所有神经网络的核心包 autograd ，也就是自动微分，了解完这些内容我们就可以较好地理解PyTorch代码了。\n",
    "#### 2.1.1 简介\n",
    "几何代数中定义的张量是基于向量和矩阵的推广，比如我们可以将标量视为零阶张量，矢量可以视为一阶张量，矩阵就是二阶张量。\n",
    "- 0维张量/**标量** 标量是一个数字\n",
    "- 1维张量/**向量** 1维张量称为“向量”。\n",
    "- 2维张量 2维张量称为**矩阵**\n",
    "- 3维张量 公用数据存储在张量 时间序列数据 股价 文本数据 彩色图片(**RGB**)\n",
    "张量是现代机器学习的基础。它的核心是一个数据容器，多数情况下，它包含数字，有时候它也包含字符串，但这种情况比较少。因此可以把它想象成一个数字的水桶。\n",
    "\n",
    "这里有一些存储在各种类型张量的公用数据集类型：\n",
    "- 3维 = 时间序列\n",
    "\n",
    "- 4维 = 图像\n",
    "\n",
    "- 5维 = 视频\n",
    "\n",
    "例子：一个图像可以用三个字段表示：\n",
    "\n",
    "(width, height, channel) = 3D\n",
    "\n",
    "但是，在机器学习工作中，我们经常要处理不止一张图片或一篇文档——我们要处理一个集合。我们可能有10,000张郁金香的图片，这意味着，我们将用到4D张量：\n",
    "\n",
    "(sample_size, width, height, channel) = 4D\n",
    "\n",
    "在PyTorch中， torch.Tensor 是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现 Tensor 和NumPy的多维数组非常类似。然而，Tensor 提供GPU计算和自动求梯度等更多功能，这些使 Tensor 这一数据类型更加适合深度学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 创建tensor\n",
    "在接下来的内容中，我们将介绍几种创建tensor的方法。\n",
    "\n",
    "   1.我们可以通过 torch.rand() 的方法，构造一个随机初始化的矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4496, 0.7625, 0.2019],\n",
      "        [0.4191, 0.7419, 0.9079],\n",
      "        [0.7936, 0.3753, 0.6504],\n",
      "        [0.9164, 0.4621, 0.6002]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(4,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.我们可以通过 torch.zeros() 构造一个矩阵全为 0，并且通过dtype设置数据类型为 long。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4,3,dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.我们可以通过 torch.tensor() 直接使用数据，构造一个张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.基于已经存在的tensor，创建一个tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.4823,  1.3848,  1.0903],\n",
      "        [-0.8851, -1.1822,  0.0688],\n",
      "        [-0.1351,  0.0219,  0.5621],\n",
      "        [-1.3599,  0.8541,  1.9548]])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(4,3,dtype=torch.double)\n",
    "# 创建一个新的tensor，返回的tensor默认具有相同的 torch.dtype和torch.device\n",
    "# 也可以像之前的写法 x = torch.ones(4, 3, dtype=torch.double)\n",
    "print(x)\n",
    "x = torch.randn_like(x,dtype=torch.float)\n",
    "print(x)\n",
    "# 结果会有一样的size\n",
    "# 获取它的维度信息\n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "# 返回的torch.Size其实是一个tuple，⽀持所有tuple的操作。\n",
    "# 还有一些常见的构造Tensor的函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一些常见的构造Tensor的函数：\n",
    "\n",
    "| 函数                | 功能                                              |\n",
    "| ------------------- | :------------------------------------------------ |\n",
    "| Tensor(sizes)       | 基础构造函数                                      |\n",
    "| tensor(data)        | 类似于np.array                                    |\n",
    "| ones(sizes)         | 全1                                               |\n",
    "| zeros(sizes)        | 全0                                               |\n",
    "| eye(sizes)          | 对角为1，其余为0                                  |\n",
    "| arange(s,e,step)    | 从s到e，步长为step                                |\n",
    "| linspace(s,e,steps) | 从s到e，均匀分成step份                            |\n",
    "| rand/randn(sizes)   | rand是[0,1)均匀分布；randn是服从N(0，1)的正态分布 |\n",
    "| normal(mean,std)    | 正态分布(均值为mean，标准差是std)                 |\n",
    "| randperm(m)         | 随机排列                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 张量的操作\n",
    "在接下来的内容中，我们将介绍几种常见的张量的操作方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.加法操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2621,  1.5037,  1.4080],\n",
      "        [-0.8104, -1.0149,  0.1433],\n",
      "        [ 0.7899,  0.1024,  1.3994],\n",
      "        [-0.8468,  1.2784,  2.8866]])\n",
      "tensor([[ 1.2621,  1.5037,  1.4080],\n",
      "        [-0.8104, -1.0149,  0.1433],\n",
      "        [ 0.7899,  0.1024,  1.3994],\n",
      "        [-0.8468,  1.2784,  2.8866]])\n",
      "tensor([[ 1.2621,  1.5037,  1.4080],\n",
      "        [-0.8104, -1.0149,  0.1433],\n",
      "        [ 0.7899,  0.1024,  1.3994],\n",
      "        [-0.8468,  1.2784,  2.8866]])\n",
      "tensor([[ 1.2621,  1.5037,  1.4080],\n",
      "        [-0.8104, -1.0149,  0.1433],\n",
      "        [ 0.7899,  0.1024,  1.3994],\n",
      "        [-0.8468,  1.2784,  2.8866]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\torch_py37\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: An output with one or more elements was resized since it had shape [6, 3], which does not match the required output shape [4, 3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ..\\aten\\src\\ATen\\native\\Resize.cpp:23.)\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "y = torch.rand(4,3)\n",
    "print(x+y)\n",
    "\n",
    "#2\n",
    "print(torch.add(x,y))\n",
    "\n",
    "#3\n",
    "result = torch.empty(6,3)\n",
    "# print(result)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)\n",
    "\n",
    "#4\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.索引操作：（类似于numpy）\n",
    "需要注意的是：索引出来的结果与原数据共享内存，修改一个，另一个会跟着修改。如果不想修改，可以考虑使用copy()等方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4823,  1.3848,  1.0903],\n",
      "        [-0.8851, -1.1822,  0.0688],\n",
      "        [-0.1351,  0.0219,  0.5621],\n",
      "        [-1.3599,  0.8541,  1.9548]])\n",
      "tensor([ 1.3848, -1.1822,  0.0219,  0.8541])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4823, 2.3848, 2.0903])\n",
      "tensor([2.4823, 3.3848, 3.0903])\n",
      "tensor([2.4823, 3.3848, 3.0903])\n"
     ]
    }
   ],
   "source": [
    "y = x[0,:]\n",
    "print(y)\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0,:]) # 源tensor也被改了了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变大小:如果你想改变一个 tensor 的大小或者形状，你可以使用 torch.view："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8) # -1是指这一维的维数由其他维度决定\n",
    "print(x.size(),y.size(),z.size())\n",
    "# 注意 view() 返回的新tensor与源tensor共享内存(其实是同一个tensor)，\n",
    "# 也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察⻆度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.8405,  0.8871,  0.4626,  1.9604],\n",
      "        [ 2.1525,  0.9405,  3.2582,  1.7831],\n",
      "        [ 3.0181,  1.3975,  2.3614,  1.4624],\n",
      "        [ 1.3257,  0.7545,  1.7155, -0.1274]])\n",
      "tensor([ 3.8405,  0.8871,  0.4626,  1.9604,  2.1525,  0.9405,  3.2582,  1.7831,\n",
      "         3.0181,  1.3975,  2.3614,  1.4624,  1.3257,  0.7545,  1.7155, -0.1274])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y) # 也加了了1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以如果我们想返回一个真正新的副本(即不共享内存)该怎么办呢？Pytorch还提供了一 个 reshape() 可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用 clone 创造一个副本然后再使用 view 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "0.6912593841552734\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# 注意：使用 clone 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor 。\n",
    "# 如果你有一个元素 tensor ，使用 .item() 来获得这个 value：\n",
    "x = torch.randn(1)\n",
    "print(type(x))\n",
    "print(x.item())\n",
    "print(type(x.item()))\n",
    "# PyTorch中的 Tensor 支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考官方文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 广播机制\n",
    "当对两个形状不同的 Tensor 按元素运算时，可能会触发广播(broadcasting)机制：先适当复制元素使这两个 Tensor 形状相同后再按元素运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)\n",
    "# 由于 x 和 y 分别是1行2列和3行1列的矩阵，如果要计算 x + y ，\n",
    "# 那么 x 中第一行的2个元素被广播 (复制)到了第二行和第三行，\n",
    "# ⽽ y 中第⼀列的3个元素被广播(复制)到了第二列。如此，就可以对2 个3行2列的矩阵按元素相加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 自动求导\n",
    "PyTorch 中，所有神经网络的核心是 `autograd `包。autograd包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义 ( define-by-run ）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的。\n",
    "\n",
    "`torch.Tensor `是这个包的核心类。如果设置它的属性` .requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用` .backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性。\n",
    "\n",
    "注意：在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。\n",
    "\n",
    "要阻止一个张量被跟踪历史，可以调用`.detach()`方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。为了防止跟踪历史记录(和使用内存），可以将代码块包装在 `with torch.no_grad(): `中。在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。\n",
    "\n",
    "还有一个类对于`autograd`的实现非常重要：`Function`。`Tensor `和` Function` 互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。每个张量都有一个`.grad_fn`属性，该属性引用了创建 `Tensor `自身的`Function`(除非这个张量是用户手动创建的，即这个张量的`grad_fn`是 `None` )。下面给出的例子中，张量由用户手动创建，因此grad_fn返回结果是None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.randn(3,3,requires_grad=True)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要计算导数，可以在 `Tensor` 上调用 `.backward()`。如果` Tensor` 是一个标量(即它包含一个元素的数据），则不需要为 `backward() `指定任何参数，但是如果它有更多的元素，则需要指定一个`gradient`参数，该参数是形状匹配的张量。\n",
    "\n",
    "创建一个张量并设置`requires_grad=True`用来追踪其计算历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对这个张量做一次运算：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y是计算的结果，所以它有`grad_fn`属性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x0000023C6A54CF08>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对 y 进行更多操作\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<MulBackward0>) tensor(3., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.requires_grad_(...) `原地改变了现有张量的`requires_grad`标志。如果没有指定的话，默认输入的这个标志是` False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000023C6A508948>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度\n",
    "\n",
    "现在开始进行反向传播，因为` out` 是一个标量，因此`out.backward()`和` out.backward(torch.tensor(1.))` 等价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出导数 `d(out)/dx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学上，若有向量函数\n",
    "\n",
    "$$ \n",
    "\\vec{y}=f(\\vec{x})\n",
    "$$\n",
    "\n",
    "，那么 \n",
    "$$ \n",
    "\\vec{y}\\)\n",
    "$$ \n",
    "关于 \n",
    "$$\n",
    "\\(\\vec{x} \n",
    "\n",
    "$$\n",
    "的梯度就是一个雅可比矩阵： \\( J=\\left(\\begin{array}{ccc}\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right) \\) 而 torch.autograd 这个包就是用来计算一些雅可比矩阵的乘积的。例如，如果 \\(v\\) 是一个标量函数 \\(l = g(\\vec{y})\\) 的梯度： \\( v=\\left(\\begin{array}{lll}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right) \\) 由链式法则，我们可以得到： \\( v J=\\left(\\begin{array}{lll}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)\\left(\\begin{array}{ccc}\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right)=\\left(\\begin{array}{lll}\\frac{\\partial l}{\\partial x_{1}} & \\cdots & \\frac{\\partial l}{\\partial x_{n}}\\end{array}\\right) \\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播⼀一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来看一个雅可比向量积的例子：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9808, -0.3108, -1.1110], requires_grad=True)\n",
      "tensor([ 1004.3168,  -318.2760, -1137.6276], grad_fn=<MulBackward0>)\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i = i + 1\n",
    "print(y)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这种情况下，`y `不再是标量。`torch.autograd` 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 `backward：`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以通过将代码块包装在` with torch.no_grad():` 中，来阻止 autograd 跟踪设置了`.requires_grad=True`的张量的历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录(即不会影响反向传播)， 那么我们可以对 tensor.data 进行操作。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值 \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 并行计算简介\n",
    "在利用PyTorch做深度学习的过程中，可能会遇到数据量较大无法在单块GPU上完成，或者需要提升计算速度的场景，这时就需要用到并行计算。本节让我们来简单地了解一下并行计算的基本概念和主要实现方式，具体的内容会在课程的第二部分详细介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 为什么要做并行计算\n",
    "\n",
    "`CUDA`是我们使用GPU的提供商——NVIDIA提供的GPU并行计算框架。对于GPU本身的编程，使用的是`CUDA`语言来实现的。但是，在我们使用PyTorch编写深度学习代码时，使用的`CUDA`又是另一个意思。在PyTorch使用 `CUDA`表示要开始要求我们的模型或者数据开始使用GPU了。\n",
    "\n",
    "在编写程序中，当我们使用了 `.cuda` 时，其功能是让我们的模型或者数据迁移到GPU当中，通过GPU开始计算。\n",
    "\n",
    "注：\n",
    "\n",
    "1. 我们使用GPU时使用的是.cuda而不是使用.gpu。这是因为当前GPU的编程接口采用CUDA，但是市面上的GPU并不是都支持CUDA，只有部分NVIDIA的GPU才支持，AMD的GPU编程接口采用的是OpenGL，在现阶段PyTorch并不支持。\n",
    "2. 数据在GPU和CPU之间进行传递时会比较耗时，应当尽量避免。\n",
    "3. GPU运算很快，但是在使用简单的操作时，我们应该尽量使用CPU去完成。\n",
    "4. 当我们的服务器上有多个GPU，我们应该指明我们使用的GPU是哪一块，如果我们不设置的话，tensor.cuda()方法会默认将tensor保存到第一块GPU上，等价于tensor.cuda(0)，这将会导致爆出`out of memory`的错误。我们可以通过以下两种方式继续设置。\n",
    "\n",
    "\n",
    "1. ```\n",
    "    #设置在文件最开始部分\n",
    "   import os\n",
    "   os.environ[\"CUDA_VISIBLE_DEVICE\"] = \"2\" # 设置默认的显卡\n",
    "   ```\n",
    "\n",
    "2. ```\n",
    "    CUDA_VISBLE_DEVICE=0,1 python train.py # 使用0，1两块GPU\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #设置在文件最开始部分\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICE\"] = \"2\" # 设置默认的显卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISBLE_DEVICE =0,1 python train.py # 使用0，1两块GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 常见的并行的方法：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**网络结构分布到不同的设备中(Network partitioning)**\n",
    "\n",
    "在刚开始做模型并行的时候，这个方案使用的比较多。其中主要的思路是，将一个模型的各个部分拆分，然后将不同的部分放入到GPU来做不同任务的计算。其架构如下\n",
    "\n",
    "![](2022-05-18-00-30-51.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**同一层的任务分布到不同数据中(Layer-wise partitioning)**\n",
    "\n",
    "第二种方式就是，同一层的模型做一个拆分，让不同的GPU去训练同一层模型的部分任务。其架构如下\n",
    "\n",
    "![](2022-05-18-00-31-55.png)\n",
    "\n",
    "这样可以保证在不同组件之间传输的问题，但是在我们需要大量的训练，同步任务加重的情况下，会出现和第一种方式一样的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**不同的数据分布到不同的设备中，执行相同的任务(Data parallelism)**\n",
    "\n",
    "第三种方式有点不一样，它的逻辑是，我不再拆分模型，我训练的时候模型都是一整个模型。但是我将输入的数据拆分。所谓的拆分数据就是，同一个模型在不同GPU中训练一部分数据，然后再分别计算一部分数据之后，只需要将输出的数据做一个汇总，然后再反传。其架构如下：\n",
    "\n",
    "![](2022-05-18-00-32-52.png)\n",
    "\n",
    "这种方式可以解决之前模式遇到的通讯问题。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS:现在的主流方式是数据并行的方式(Data parallelism)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f68a7a19ceb69147e2b1806248225db4b52a6106a01cc4bb6cd26c7074c6480"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch_py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
